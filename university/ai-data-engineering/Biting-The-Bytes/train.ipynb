{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biting The Bytes: Transformers For Diacritic Restoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Istanbul Technical University - Natural Language Processing - Spring 2024 </b>\n",
    "\n",
    "Emircan Erol - 150200324 <br>\n",
    "erole20@itu.edu.tr\n",
    "\n",
    "Muhammed RÃ¼ÅŸen Birben - 150220755 <br>\n",
    "birben20@itu.edu.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook has been run many times to get the results. We've iteratively selected checkpoint models (saved the best ones), modified the hyperparameters, and changed the dataset. The base model is [`google-byt5-small`](https://huggingface.co/google/byt5-small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resources used in this projects can be found in the following drive folder:\n",
    "\n",
    "[` Google Drive Link ğŸ—‚ï¸`](https://drive.google.com/drive/folders/1nfAvnj_-EB4FMa83mUCtGNel9DkdC0PL?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "from os.path import exists as is_path_exists\n",
    "\n",
    "LR = 2 ** (-12)\n",
    "CHECKPOINT = None # 'models/peft32-train32-256-base-trtw1e-train-512-1024-base-trw-6+kelimeler-2-6200+6300tr3600-train128tr128-1000.pt'\n",
    "MIN_LEN = 0\n",
    "MAX_LEN = 32\n",
    "model_id = 'google/byt5-small' # needed if training from scratch\n",
    "BATCH_SIZE = 64\n",
    "enc = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint None does not exist. Training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/byt5-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,657,282 || all params: 417,363,844 || trainable%: 0.6366823667648605\n"
     ]
    }
   ],
   "source": [
    "if CHECKPOINT:\n",
    "    assert is_path_exists(CHECKPOINT)\n",
    "    print(f'Checkpoint {CHECKPOINT} exists. Loading model from checkpoint.')\n",
    "    model = torch.load(CHECKPOINT)\n",
    "    # turn on peft\n",
    "    #model = prepare_model_for_kbit_training(model)\n",
    "    #model = get_peft_model(model, peft_config)\n",
    "else:\n",
    "    # peft is used to redce the trainable parameter size of the base model\n",
    "    from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "    from turbot5 import T5ForTokenClassification\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    LORA_ALPHA = 64\n",
    "    LORA_DROPOUT = 0.125\n",
    "    R = 32\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        r=R,\n",
    "        bias=\"none\",\n",
    "        task_type='TOKEN_CLS',\n",
    "        target_modules = ['k', 'q', 'v', 'o']\n",
    "    )\n",
    "    print(f'Checkpoint {CHECKPOINT} does not exist. Training from scratch.')\n",
    "    \n",
    "    model = T5ForTokenClassification.from_pretrained(model_id,\n",
    "                                                    num_labels=2,\n",
    "                                                    torch_dtype=torch.bfloat16,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    attention_type = 'flash')\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the conversion of main training data to .json is not provided, it is very similar with the conversion of the text, just substitute the diactritic characters with their non-diactritic counterparts. and convert to .json.\n",
    "\n",
    "Something like this:\n",
    "\n",
    "```python\n",
    "\n",
    "def process_data(input_file, output_file, answer_file):\n",
    "    # Read input file (CSV or JSONL)\n",
    "    if input_file.endswith('.csv'):\n",
    "        df = pd.read_csv(input_file)\n",
    "    elif input_file.endswith('.jsonl'):\n",
    "        df = pd.read_json(input_file, lines=True, encoding='utf-8-sig')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input file format. Use CSV or JSONL.\")\n",
    "\n",
    "    # Shuffle the dataframe\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'Sentence': 'output'})\n",
    "\n",
    "\n",
    "    # Add an input non-diacritic version of the output\n",
    "    # This is the input for the model\n",
    "    df['input'] = df['output'].apply(asciify_turkish_chars)    \n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[['ID', 'input', 'output']]\n",
    "\n",
    "    # Save the processed data to the output file (JSONL)\n",
    "    df.to_json(output_file, orient='records', lines=True)\n",
    "\n",
    "    # Save the answer data to the answer file (CSV)\n",
    "    df[['ID', 'output']].to_csv(answer_file, index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asciify_turkish_chars(text):\n",
    "    \"\"\"\n",
    "    Removes diacritics from Turkish text.\n",
    "    Args:\n",
    "        text (str): Turkish text.\n",
    "    Returns:\n",
    "        text: Turkish text without diacritics.\n",
    "    \"\"\"\n",
    "    turkish_chars = {\n",
    "        'Ã§': 'c',\n",
    "        'ÄŸ': 'g',\n",
    "        'Ä±': 'i',\n",
    "        'Ã¶': 'o',\n",
    "        'ÅŸ': 's',\n",
    "        'Ã¼': 'u',\n",
    "        'Ã‡': 'C',\n",
    "        'Ä': 'G',\n",
    "        'Ä°': 'I',\n",
    "        'Ã–': 'O',\n",
    "        'Å': 'S',\n",
    "        'Ãœ': 'U'\n",
    "    }\n",
    "    \n",
    "    for char, replacement in turkish_chars.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def txt_to_input_output(fp, skip=500_000, split='all'):\n",
    "    \"\"\"\n",
    "    Reads a text file and writes it to a jsonl file. \n",
    "    So that it can be used as a dataset.\n",
    "    Args:\n",
    "        fp (str): File path of the text file.\n",
    "        skip (int): Number of lines to skip.\n",
    "        split (int): Number of lines to read.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    dict_data = []\n",
    "    with open(fp, 'r') as f:\n",
    "        for line in f:\n",
    "            if skip:\n",
    "                skip -= 1\n",
    "                continue\n",
    "            \n",
    "            elif line == '\\n': continue\n",
    "\n",
    "            elif split == 'all':\n",
    "                sentences.append(line.strip('\\n'))\n",
    "\n",
    "            elif MIN_LEN <= len(line) <= MAX_LEN:\n",
    "                sentences.append(line.strip('\\n'))\n",
    "                split -= 1\n",
    "                if split == 0: break\n",
    "\n",
    "        dict_data = [{'input': asciify_turkish_chars(i), 'output': i} for i in sentences]\n",
    "\n",
    "    with open('data.jsonl', mode='w', encoding='utf-8') as f:\n",
    "        for w_dict in dict_data:\n",
    "            json.dump(w_dict, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "# running the function if we need to create dataset from scratch\n",
    "# txt_to_input_output('turkce_kelime_listesi.txt', skip=0, split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 76186\n"
     ]
    }
   ],
   "source": [
    "# Read data with a minimum and maximum length\n",
    "path = 'data.jsonl' # 'data.jsonl'\n",
    "\n",
    "data = []\n",
    "with open(path, 'r', encoding=enc) as f:\n",
    "    for jline in f.readlines():\n",
    "        line = json.loads(jline)\n",
    "        n = len(line['input'])\n",
    "        if MIN_LEN <= n <= MAX_LEN:\n",
    "            data.append(line)\n",
    "\n",
    "print(f'Number of samples: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ovunmak\n",
      "Output: ovunmak\n",
      "Input: mujdecilik\n",
      "Output: mÃ¼jdecilik\n",
      "Input: Ardesen \n",
      "Output: ArdeÅŸen \n",
      "Input: bir alay\n",
      "Output: bir alay\n",
      "Input: ihtiyat akcesi\n",
      "Output: ihtiyat akÃ§esi\n"
     ]
    }
   ],
   "source": [
    "# print some samples\n",
    "for i in range(5):\n",
    "    print(f\"Input: {data[i]['input']}\\nOutput: {data[i]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 1190\n"
     ]
    }
   ],
   "source": [
    "def mask_label(data, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Masks the padded tokens in the input and creates batches.\n",
    "    Args:\n",
    "        data (list): List of dictionaries.\n",
    "        batch_size (int): Batch size.\n",
    "    Returns:\n",
    "        batches (list): List of dictionaries.\n",
    "        The dictionaries contain the following keys: 'input_ids', 'labels', 'attention_mask'\n",
    "    \"\"\"\n",
    "    batches = list()\n",
    "    \n",
    "    dataset = list()\n",
    "    for idx, sample in enumerate(data):\n",
    "        inp = list(sample['input'])\n",
    "        gold = list(sample['output'])\n",
    "        \n",
    "        new_sample = dict()\n",
    "\n",
    "        label = []\n",
    "        \n",
    "        # create labels\n",
    "        for i, j in zip(inp, gold):\n",
    "            n = len(i.encode('utf-8'))\n",
    "            label.extend([i != j] * n)\n",
    "\n",
    "        new_sample['labels'] = label #Â labels\n",
    "        new_sample['input_ids'] = [i + 3 for i in sample['input'].encode('utf-8')] #Â input_ids\n",
    "        \n",
    "        assert len(label) == len(new_sample['input_ids'])\n",
    "        new_sample['input_ids'].append(0) # eos token\n",
    "\n",
    "        dataset.append(new_sample)\n",
    "        if idx and not (idx % batch_size):\n",
    "            batch = dict()\n",
    "            max_size = 0\n",
    "            for i in dataset[idx-batch_size: idx]:\n",
    "                length = len(i['input_ids'])\n",
    "                if length > max_size: max_size = length\n",
    "\n",
    "            input_ids = list()\n",
    "            labels = list()\n",
    "\n",
    "            # padding\n",
    "            for i in dataset[idx-batch_size: idx]:\n",
    "                i['labels'].extend([False] * (max_size - len(i['labels'])))\n",
    "                i['input_ids'].extend([0] * (max_size - len(i['input_ids'])))\n",
    "                input_ids.append(i['input_ids'])\n",
    "                labels.append(i['labels'])\n",
    "\n",
    "            batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "            batch['input_ids'] = torch.tensor(input_ids, dtype=torch.int64)\n",
    "\n",
    "            # attention mask: 1 for real tokens, 0 for padding\n",
    "            batch['attention_mask'] = torch.ones_like(batch['input_ids'], dtype=torch.int64)\n",
    "            batch['attention_mask'][batch['input_ids'] == 0] = 0\n",
    "            batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "ds = mask_label(data)\n",
    "print(f'Number of batches: {len(ds)}')\n",
    "assert len(ds) > 0\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def test_mask(data):\n",
    "    \"\"\"\n",
    "    Masks the padded tokens in the input.\n",
    "    Args:\n",
    "        data (list): List of strings.\n",
    "    Returns:\n",
    "        dataset (list): List of dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = list()\n",
    "    for sample in data:        \n",
    "        new_sample = dict()\n",
    "\n",
    "        input_tokens = [i + 3 for i in sample.encode('utf-8')]\n",
    "        input_tokens.append(0) # eos token\n",
    "        new_sample['input_ids'] = torch.tensor([input_tokens], dtype=torch.int64)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_tokens)  # Attend to all tokens\n",
    "        new_sample['attention_mask'] = torch.tensor([attention_mask], dtype=torch.int64)\n",
    "        \n",
    "        dataset.append(new_sample)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def rewrite(model, data):\n",
    "    \"\"\"\n",
    "    Rewrites the input text with the model.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        data (dict): Dictionary containing 'input_ids' and 'attention_mask'.\n",
    "    Returns:\n",
    "        output (str): Rewritten text.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k, v in data.items():\n",
    "            data[k] = data[k].to(model.device)\n",
    "        pred = torch.argmax(model(**data).logits, dim=2)\n",
    "        \n",
    "    output = list() # save the indices of the characters as list of integers\n",
    "    \n",
    "    # Conversion table for Turkish characters {100: [300, 350], ...}\n",
    "    en2tr = {en: tr for tr, en in zip(list(map(list, map(str.encode, list('ÃœÄ°ÄÅÃ‡Ã–Ã¼Ä±ÄŸÅŸÃ§Ã¶')))), list(map(ord, list('UIGSCOuigsco'))))}\n",
    "\n",
    "    for inp, lab in zip((data['input_ids'] - 3)[0].tolist(), pred[0].tolist()):\n",
    "        if lab and inp in en2tr:\n",
    "            # if the model predicts a diacritic, replace it with the corresponding Turkish character\n",
    "            output.extend(en2tr[inp])\n",
    "        elif inp >= 0: output.append(inp)\n",
    "    return bytes(output).decode()\n",
    "\n",
    "def submission(model=model, split=None, path='test.jsonl', output_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    Creates a submission file.\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model.\n",
    "        path (str): Path of the test data.\n",
    "        output_path (str): Path of the submission file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Dataframe containing the submission.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = list()\n",
    "    with open(path, 'r', encoding=enc) as f:\n",
    "        data = [json.loads(jline) for jline in f.readlines()[:split]]\n",
    "        ds = test_mask([i['input'] for i in data])\n",
    "        for sample in tqdm(ds):\n",
    "            predictions.append([rewrite(model, sample)])\n",
    "    \n",
    "    df = pd.DataFrame(predictions, columns=['output'])\n",
    "\n",
    "    # take ids from data\n",
    "    df['ID'] = [i['ID'] for i in data]\n",
    "    df.rename(columns={'output': 'Sentence'}, inplace=True)\n",
    "    df = df[['ID', 'Sentence']]\n",
    "    df.to_csv(output_path, index=False, encoding=enc)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def acc_overall(test_result, testgold, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculates the overall accuracy. Using the formula used in the competition.\n",
    "    Args:\n",
    "        test_result (list): List of strings.\n",
    "        testgold (list): List of strings.\n",
    "        verbose (bool): If True, prints the incorrect words.\n",
    "    Returns:\n",
    "        float: Overall accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # count number of correctly diacritized words\n",
    "    for i in range(len(testgold)):\n",
    "        golds = testgold[i].split()\n",
    "        results = test_result[i].split()\n",
    "        for m in range(len(golds)):\n",
    "            if results[m] == golds[m]:\n",
    "                correct += 1\n",
    "            elif verbose:\n",
    "                print(results[m], golds[m]) #Â print the incorrect words, handy to see the errors\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*setting weights and biases service for tracking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\emirc\\Desktop\\NLP Project\\wandb\\run-20240506_192946-v8nrjjhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe' target=\"_blank\">pious-aardvark-94</a></strong> to <a href='https://wandb.ai/itu-nlp/NLP%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itu-nlp/NLP%20Project' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"NLP Project\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"true\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"token_classification.ipynb\"\n",
    "run = wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243e844adb8049048f110e6525d884e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.133 MB of 0.133 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>â–„â–ƒâ–†â–„â–‚â–„â–…â–‚â–…â–‡â–‚â–ƒâ–…â–„â–†â–†â–…â–†â–‡â–…â–â–…â–…â–†â–„â–ˆâ–†â–†â–‚â–‡â–„â–„â–†â–ƒâ–‡â–†â–…â–ƒâ–ƒâ–…</td></tr><tr><td>batch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>f1</td><td>â–‚â–ƒâ–„â–â–‚â–…â–„â–‚â–†â–†â–‚â–„â–â–„â–…â–…â–†â–‡â–†â–„â–â–ƒâ–„â–„â–‚â–†â–…â–‡â–ƒâ–ˆâ–†â–†â–†â–ƒâ–‡â–†â–‡â–„â–ƒâ–‡</td></tr><tr><td>loss</td><td>â–‡â–ƒâ–†â–†â–ˆâ–ƒâ–…â–…â–‚â–„â–…â–†â–…â–†â–…â–†â–‚â–„â–‚â–†â–‡â–…â–‡â–„â–…â–†â–…â–„â–…â–â–‚â–ƒâ–‚â–„â–â–ƒâ–‚â–‚â–„â–</td></tr><tr><td>precision</td><td>â–ƒâ–†â–ƒâ–‚â–†â–‡â–…â–‡â–†â–„â–…â–†â–â–†â–…â–„â–†â–†â–„â–„â–‡â–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–‡â–‡â–†â–ˆâ–ˆâ–…â–…â–†â–†â–‡â–‡â–…â–‡</td></tr><tr><td>recall</td><td>â–„â–ƒâ–†â–„â–‚â–„â–…â–‚â–…â–‡â–‚â–ƒâ–…â–„â–†â–†â–…â–†â–‡â–…â–â–…â–…â–†â–„â–ˆâ–†â–†â–‚â–‡â–„â–„â–†â–ƒâ–‡â–†â–…â–ƒâ–ƒâ–…</td></tr><tr><td>validation score</td><td>â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.85366</td></tr><tr><td>batch</td><td>940</td></tr><tr><td>f1</td><td>0.875</td></tr><tr><td>loss</td><td>0.04966</td></tr><tr><td>precision</td><td>0.89744</td></tr><tr><td>recall</td><td>0.85366</td></tr><tr><td>validation score</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-aardvark-94</strong> at: <a href='https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project/runs/v8nrjjhe</a><br/> View project at: <a href='https://wandb.ai/itu-nlp/NLP%20Project' target=\"_blank\">https://wandb.ai/itu-nlp/NLP%20Project</a><br/>Synced 5 W&B file(s), 4 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240506_192946-v8nrjjhe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "epoch = 1\n",
    "val_steps = 250\n",
    "for i in range(epoch):\n",
    "    for counter, batch in tqdm(enumerate(ds)):\n",
    "\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = batch[k].to('cuda')\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        # Compute categorical cross-entropy loss\n",
    "        active_loss = attention_mask.view(-1) == 1 # select only non-padded tokens (attended tokens)\n",
    "        active_logits = logits.view(-1, logits.shape[-1]) # get logits for all tokens\n",
    "        active_labels = torch.where(\n",
    "            active_loss, batch['labels'].view(-1), torch.tensor(model.config.pad_token_id).type_as(batch['labels'])\n",
    "        ) \n",
    "        loss = F.cross_entropy(active_logits, active_labels) # compute loss\n",
    "        \n",
    "        # Compute metrics\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "        tp = ((batch['labels'] == predictions) & (batch['labels'] != model.config.pad_token_id)).sum()\n",
    "        tn = ((batch['labels'] != predictions) & (batch['labels'] != model.config.pad_token_id) & (predictions != model.config.pad_token_id)).sum()\n",
    "        total_labels = (batch['labels'] != model.config.pad_token_id).sum()\n",
    "        recall = tp / total_labels\n",
    "        acc = (tp + tn) / total_labels\n",
    "        precision = tp / (predictions != model.config.pad_token_id).sum()\n",
    "        f1 = (2 * recall * precision) / (recall + precision)\n",
    "\n",
    "        del batch #Â free memory\n",
    "\n",
    "        # Step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        run.log({\"loss\": loss.item(), \"batch\": counter+1, 'recall': recall, 'accuracy': acc, 'precision': precision, 'f1': f1})\n",
    "        print(f'Epoch: [{i+1}/{epoch}], Batch [{counter+1}/{len(ds)}], Loss: {loss.item():.4f}, F1: {f1:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, Accuracy: {acc:.4f}')\n",
    "\n",
    "        if not (counter % val_steps): #Â validation\n",
    "            torch.save(model.state_dict(), f'base-{counter}.pt')\n",
    "            submission_df = submission(path='data/valid.jsonl', output_path='submission.csv')\n",
    "            answers = pd.read_csv('submission.csv')\n",
    "            val_score = acc_overall(submission_df.Sentence, answers.Sentence)\n",
    "            run.log({\"validation\": wandb.Table(dataframe=submission_df), 'validation score': val_score})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*save the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'base_kelimeler-2.pt') # PEFT save function had a bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 47.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001147</td>\n",
       "      <td>limeler ince Ã¼nlÃ¼ bulunduran bir fiille birlik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002343</td>\n",
       "      <td>bu yÄ±l tÃ¼rkiye olarak dÃ¶rdÃ¼ncÃ¼ kez katÄ±lacaÄŸÄ±m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005460</td>\n",
       "      <td>bizim gÃ¶rÃ¼ÅŸÃ¼mÃ¼z ve uygulamalarÄ±mÄ±zda nusret f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002100</td>\n",
       "      <td>ankaralÄ±lar mÄ±sÄ±ra popcorn demez ve onu bir f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005445</td>\n",
       "      <td>daha sonralarÄ± iÃ§ iÃ§e bir yaÅŸam sÃ¼rdÃ¼receÄŸim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1007513</td>\n",
       "      <td>sizce bir faydasÄ± olur mu ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1004524</td>\n",
       "      <td>yalÃ§Ä±nkaya : bu yatÄ±rÄ±m sepeti kasÄ±m ayÄ±nda 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1005141</td>\n",
       "      <td>Ã§ok yÃ¼ksek : filmlerin romanlarÄ±n mÃ¼thiÅŸ bilg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1003252</td>\n",
       "      <td>yenilenen stili ve kullanÄ±lan yeni teknolojile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1003791</td>\n",
       "      <td>antalyaÃ§ok bulutlu22 Â°c / 15 Â°c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                           Sentence\n",
       "0   1001147  limeler ince Ã¼nlÃ¼ bulunduran bir fiille birlik...\n",
       "1   1002343  bu yÄ±l tÃ¼rkiye olarak dÃ¶rdÃ¼ncÃ¼ kez katÄ±lacaÄŸÄ±m...\n",
       "2   1005460   bizim gÃ¶rÃ¼ÅŸÃ¼mÃ¼z ve uygulamalarÄ±mÄ±zda nusret f...\n",
       "3   1002100   ankaralÄ±lar mÄ±sÄ±ra popcorn demez ve onu bir f...\n",
       "4   1005445   daha sonralarÄ± iÃ§ iÃ§e bir yaÅŸam sÃ¼rdÃ¼receÄŸim ...\n",
       "..      ...                                                ...\n",
       "95  1007513                       sizce bir faydasÄ± olur mu ? \n",
       "96  1004524   yalÃ§Ä±nkaya : bu yatÄ±rÄ±m sepeti kasÄ±m ayÄ±nda 6...\n",
       "97  1005141   Ã§ok yÃ¼ksek : filmlerin romanlarÄ±n mÃ¼thiÅŸ bilg...\n",
       "98  1003252  yenilenen stili ve kullanÄ±lan yeni teknolojile...\n",
       "99  1003791                   antalyaÃ§ok bulutlu22 Â°c / 15 Â°c \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = submission(path='data/valid.jsonl', output_path='submission.csv')\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'limeler ince Ã¼nlÃ¼ bulunduran bir fiille birlikte kullanÄ±lÄ±yorsa bir ulama sonucunda son sesteki bu kalÄ±n k sesi de ince okunabilmekte ve hatta gereÄŸi yokken birleÅŸik bile yazilabilmektedir . '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.Sentence[0] #Â check the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yazilabilmektedir yazÄ±labilmektedir\n",
      "ayinda ayÄ±nda\n",
      "duzenlenecek dÃ¼zenlenecek\n",
      "turkiye tÃ¼rkiye\n",
      "odul Ã¶dÃ¼l\n",
      "cifte Ã§ifte\n",
      "hakkinda hakkÄ±nda\n",
      "yazin yazÄ±n\n",
      "beg'de beÄŸ'de\n",
      "in Ä±n\n",
      "bilgisayarâ€™dir bilgisayarâ€™dÄ±r\n",
      "gecti geÃ§ti\n",
      "dun dÃ¼n\n",
      "tum tÃ¼m\n",
      "uzmanlari uzmanlarÄ±\n",
      "yapildi yapÄ±ldÄ±\n",
      "nu nÃ¼\n",
      "in Ä±n\n",
      "olmasÄ±nki olmasÄ±nkÄ±\n",
      "dÃ¶nÃ¼klÃ¼k donukluk\n",
      "kubali kÃ¼balÄ±\n",
      "onlarin onlarÄ±n\n",
      "tercumanliÄŸinÄ± tercÃ¼manlÄ±ÄŸÄ±nÄ±\n",
      "yapmis yapmÄ±ÅŸ\n",
      "yaÅŸÄ±n yasin\n",
      "geÃ§miste geÃ§miÅŸte\n",
      "bariÅŸmakta barÄ±ÅŸmakta\n",
      "gormemistir gÃ¶rmemiÅŸtir\n",
      "siÃ§ak sÄ±cak\n",
      "adÄ± adi\n",
      "dur. dÃ¼r.\n",
      "ornekti Ã¶rnekti\n",
      "aÄŸasi aÄŸasÄ±\n",
      "asÄ±retinin aÅŸiretinin\n",
      "sÄ±mdiki ÅŸimdiki\n",
      "adiyaman adÄ±yaman\n",
      "in Ä±n\n",
      "adiyaman adÄ±yaman\n",
      "in Ä±n\n",
      "yalin yalÄ±n\n",
      "bÄ±cimlerin biÃ§imlerin\n",
      "degil deÄŸil\n",
      "iÃ´ Ä±Ã´\n",
      "degil deÄŸil\n",
      "taklÄ±t taklit\n",
      "yazilmasi yazÄ±lmasÄ±\n",
      "eÅŸas esas\n",
      "alinmistir alÄ±nmÄ±ÅŸtÄ±r\n",
      "hayatin hayatÄ±n\n",
      "sacmaliklarla saÃ§malÄ±klarla\n",
      "oldugÃ¼nu olduÄŸunu\n",
      "duygularin duygularÄ±n\n",
      "unutuldugÃ¼nu unutulduÄŸunu\n",
      "m3â€™un m3â€™Ã¼n\n",
      "uzerindeki Ã¼zerindeki\n",
      "icin iÃ§in\n",
      "(hukumet (hÃ¼kÃ¼met\n",
      "olmadi olmadÄ±\n",
      "dogulu doÄŸulu\n",
      "cakkidÄ± Ã§akkÄ±dÄ±\n",
      "ÄŸibi gibi\n",
      "sarkiyla ÅŸarkÄ±yla\n",
      "ortaligÄ± ortalÄ±ÄŸÄ±\n",
      "yikar yÄ±kar\n",
      "gecer geÃ§er\n",
      "siÅŸitemi sisitemi\n",
      "ask aÅŸk\n",
      "yasamÄ± yaÅŸamÄ±\n",
      "baktiÄŸimÄ±zda baktÄ±ÄŸÄ±mÄ±zda\n",
      "savas savaÅŸ\n",
      "Ã¶nÃ§esinde Ã¶ncesinde\n",
      "sunlar ÅŸunlar\n",
      "boyle bÃ¶yle\n",
      "saskina ÅŸaÅŸkÄ±na\n",
      "donmuÅŸtum dÃ¶nmÃ¼ÅŸtÃ¼m\n",
      "balÄ±kÃ§iliÄŸÄ±na balÄ±kÃ§Ä±lÄ±ÄŸÄ±na\n",
      "sahipligÄ± sahipliÄŸi\n",
      "yapiyor yapÄ±yor\n",
      "Ã¶n on\n",
      "eÄŸrÄ±kavuk eÄŸrikavuk\n",
      "genis geniÅŸ\n",
      "rahatsiz rahatsÄ±z\n",
      "icin iÃ§in\n",
      "elestÄ±ri eleÅŸtiri\n",
      "ozellestÄ±rmeler Ã¶zelleÅŸtirmeler\n",
      "taseronlasma taÅŸeronlaÅŸma\n",
      "basortulÃ¼ler baÅŸÃ¶rtÃ¼lÃ¼ler\n",
      "unÄ±versitelerde Ã¼niversitelerde\n",
      "ozel Ã¶zel\n",
      "calisaÃ§aklar Ã§alÄ±ÅŸacaklar\n",
      "motorlarin motorlarÄ±n\n",
      "yanisÄ±ra yanÄ±sÄ±ra\n",
      "secenekleri seÃ§enekleri\n",
      "kapatiyor kapatÄ±yor\n",
      "aÃ§ik aÃ§Ä±k\n",
      "bugun bugÃ¼n\n",
      "uyeye Ã¼yeye\n",
      "kararina kararÄ±na\n",
      "degÄ±l deÄŸil\n",
      "ayni aynÄ±\n",
      "hukumete hÃ¼kÃ¼mete\n",
      "buyuk bÃ¼yÃ¼k\n",
      "oldugÃ¼ olduÄŸu\n",
      "goruÅŸunÃ¼ gÃ¶rÃ¼ÅŸÃ¼nÃ¼\n",
      "ÅŸavunurken savunurken\n",
      "muttefikleri mÃ¼ttefikleri\n",
      "surÃ¼klendiler sÃ¼rÃ¼klendiler\n",
      "seklinde ÅŸeklinde\n",
      "ayirarak ayÄ±rarak\n",
      "giris giriÅŸ\n",
      "yapinÄ±z yapÄ±nÄ±z\n",
      "cifte Ã§ifte\n",
      "yapalim yapalÄ±m\n",
      "turkiye tÃ¼rkiye\n",
      "hukumet hÃ¼kÃ¼met\n",
      "yapsin yapsÄ±n\n",
      "u Ã¼\n",
      "artÃ¼klÃ¼ artuklu\n",
      "kalÄ±ntÄ±lari kalÄ±ntÄ±larÄ±\n",
      "oÄŸÃ¼nden ogÃ¼nden\n",
      "cÃ¶k Ã§ok\n",
      "guzel gÃ¼zel\n",
      "beraberligÄ±miz beraberliÄŸimiz\n",
      "evliligÄ± evliliÄŸi\n",
      "hic hiÃ§\n",
      "duÅŸÃ¼nmuyorduk dÃ¼ÅŸÃ¼nmÃ¼yorduk\n",
      "teror terÃ¶r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9202762084118016"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = pd.read_csv('data/valid_answers.csv')\n",
    "val = acc_overall(val_df.Sentence, answers.Sentence)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission on test set - Real World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1157/1157 [00:21<00:00, 52.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tr ekonomi ve politika haberleri tÃ¼rkiye nin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ã¼ye giriÅŸi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>son gÃ¼ncelleme 12:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ä°mralÄ± Mit gÃ¶rÃ¼ÅŸmesi ihtiyaÃ§ duyuldukÃ§a oluyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Suriye deki silahlÄ± selefi muhalifler yeni ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1152</td>\n",
       "      <td>YÃ¼reÄŸir Adana ilimize ait ÅŸirin bir ilÃ§edir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1153</td>\n",
       "      <td>yÃ¼ze gÃ¼lÃ¼cÃ¼lÃ¼ÄŸÃ¼n at oynattÄ±ÄŸÄ± bir aydÄ±nlar ort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1154</td>\n",
       "      <td>zavallÄ± adamÄ± oracÄ±kta astÄ±lar ve hiÃ§ kimse se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1155</td>\n",
       "      <td>zengin Ã§ocuklarÄ±na arÄ±z mÃ¼nasebetsizlikler fak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1156</td>\n",
       "      <td>senin aÃ§Ä±n hepimizin acÄ±sÄ±dÄ±r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           Sentence\n",
       "0        0   tr ekonomi ve politika haberleri tÃ¼rkiye nin ...\n",
       "1        1                                        Ã¼ye giriÅŸi \n",
       "2        2                              son gÃ¼ncelleme 12:12 \n",
       "3        3    Ä°mralÄ± Mit gÃ¶rÃ¼ÅŸmesi ihtiyaÃ§ duyuldukÃ§a oluyor \n",
       "4        4   Suriye deki silahlÄ± selefi muhalifler yeni ku...\n",
       "...    ...                                                ...\n",
       "1152  1152       YÃ¼reÄŸir Adana ilimize ait ÅŸirin bir ilÃ§edir \n",
       "1153  1153  yÃ¼ze gÃ¼lÃ¼cÃ¼lÃ¼ÄŸÃ¼n at oynattÄ±ÄŸÄ± bir aydÄ±nlar ort...\n",
       "1154  1154  zavallÄ± adamÄ± oracÄ±kta astÄ±lar ve hiÃ§ kimse se...\n",
       "1155  1155  zengin Ã§ocuklarÄ±na arÄ±z mÃ¼nasebetsizlikler fak...\n",
       "1156  1156                     senin aÃ§Ä±n hepimizin acÄ±sÄ±dÄ±r \n",
       "\n",
       "[1157 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = submission(path='data/test.jsonl', output_path='submission.csv')\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_it(text, model=model):\n",
    "    sample = test_mask([text])\n",
    "    return rewrite(model, sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nasÄ±lsÄ±n bu akÅŸam, bir ÅŸeyler iÃ§mek ister misin'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try_it('nasilsin bu aksam, bir seyler icmek ister misin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
